{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11951f08-5f2d-4a18-8c6c-e6616e75dd55",
   "metadata": {},
   "source": [
    "# PrefixSpan\n",
    "\n",
    "频繁子序列挖掘\n",
    "\n",
    "数据集：[Groceries dataset](https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1081d2ab-f20c-41f6-b337-0f35a5e659db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:25.856890Z",
     "iopub.status.busy": "2024-09-05T08:48:25.856408Z",
     "iopub.status.idle": "2024-09-05T08:48:28.236407Z",
     "shell.execute_reply": "2024-09-05T08:48:28.235274Z",
     "shell.execute_reply.started": "2024-09-05T08:48:25.856813Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    Row,\n",
    "    functions as F\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, IntegerType, BooleanType\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dae51d-61fa-4f4d-920a-57d26eef932b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:28.238010Z",
     "iopub.status.busy": "2024-09-05T08:48:28.237605Z",
     "iopub.status.idle": "2024-09-05T08:48:28.242599Z",
     "shell.execute_reply": "2024-09-05T08:48:28.241444Z",
     "shell.execute_reply.started": "2024-09-05T08:48:28.237985Z"
    }
   },
   "outputs": [],
   "source": [
    "CSV_PATH = './data'\n",
    "CSV_FILE = 'Groceries_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c7bfd-0cc0-479b-b698-dca4e5614337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:43:03.669345Z",
     "iopub.status.busy": "2024-09-05T05:43:03.668365Z",
     "iopub.status.idle": "2024-09-05T05:43:03.673194Z",
     "shell.execute_reply": "2024-09-05T05:43:03.672421Z",
     "shell.execute_reply.started": "2024-09-05T05:43:03.669309Z"
    }
   },
   "source": [
    "## 1. 一个简单的例子\n",
    "\n",
    "数据说明：\n",
    "\n",
    "- `[[1, 2], [3]]`：1 和 2 被认为是一起发生，3 随后发生\n",
    "- `[[1, 2, 3]]`：1, 2, 3 按顺序发生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacf3b86-618a-447a-b990-b21c6d328e4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:28.243793Z",
     "iopub.status.busy": "2024-09-05T08:48:28.243474Z",
     "iopub.status.idle": "2024-09-05T08:48:40.483387Z",
     "shell.execute_reply": "2024-09-05T08:48:40.482280Z",
     "shell.execute_reply.started": "2024-09-05T08:48:28.243769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/05 16:48:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/05 16:48:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/05 16:48:37 WARN PrefixSpan: Input data is not cached.                    \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|sequence|freq|\n",
      "+--------+----+\n",
      "|   [[2]]|   3|\n",
      "|   [[3]]|   2|\n",
      "|   [[1]]|   3|\n",
      "|[[1, 2]]|   3|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 将日志级别设为 WARN\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark_df = sc.parallelize([Row(sequence=[[1, 2], [3]]),\n",
    "                           Row(sequence=[[1, 2, 3, 2, 1, 2]]),\n",
    "                           Row(sequence=[[1, 2], [5]]),\n",
    "                           Row(sequence=[[6]])]).toDF()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.5,\n",
    "                        maxPatternLength=5,\n",
    "                        maxLocalProjDBSize=32000000)\n",
    "\n",
    "# Find frequent sequential patterns.\n",
    "prefixSpan.findFrequentSequentialPatterns(spark_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb514df-68d2-4d11-a47d-519b17079482",
   "metadata": {},
   "source": [
    "## 2. 子序列挖掘\n",
    "\n",
    "### 2.1 商品编码\n",
    "\n",
    "从 CSV 读入数据，并将商品名称 `itemDescription` 转换为商品编码 `itemCode`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c660a7-7d0b-476a-8468-daaebb0198f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:40.485690Z",
     "iopub.status.busy": "2024-09-05T08:48:40.485250Z",
     "iopub.status.idle": "2024-09-05T08:48:41.450080Z",
     "shell.execute_reply": "2024-09-05T08:48:41.449257Z",
     "shell.execute_reply.started": "2024-09-05T08:48:40.485649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 16:48:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------+\n",
      "|Member_number|      Date| itemDescription|\n",
      "+-------------+----------+----------------+\n",
      "|         1808|21-07-2015|  tropical fruit|\n",
      "|         2552|05-01-2015|      whole milk|\n",
      "|         2300|19-09-2015|       pip fruit|\n",
      "|         1187|12-12-2015|other vegetables|\n",
      "|         3037|01-02-2015|      whole milk|\n",
      "|         4941|14-02-2015|      rolls/buns|\n",
      "|         4501|08-05-2015|other vegetables|\n",
      "|         3803|23-12-2015|      pot plants|\n",
      "|         2762|20-03-2015|      whole milk|\n",
      "|         4119|12-02-2015|  tropical fruit|\n",
      "+-------------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从 CSV 中读取数据，存成 Pandas DataFrame\n",
    "abs_path = utils.gen_abspath(CSV_PATH, CSV_FILE)\n",
    "df = utils.read_csv(abs_path)\n",
    "\n",
    "# 将 Pandas DataFrame 加载到 Spark\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d511d68-9eaa-4b96-a20e-69c3b583994e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:41.455418Z",
     "iopub.status.busy": "2024-09-05T08:48:41.454934Z",
     "iopub.status.idle": "2024-09-05T08:48:44.603016Z",
     "shell.execute_reply": "2024-09-05T08:48:44.601635Z",
     "shell.execute_reply.started": "2024-09-05T08:48:41.455376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------+--------+\n",
      "|Member_number|      Date| itemDescription|itemCode|\n",
      "+-------------+----------+----------------+--------+\n",
      "|         1808|2015-07-21|  tropical fruit|       6|\n",
      "|         2552|2015-01-05|      whole milk|       0|\n",
      "|         2300|2015-09-19|       pip fruit|      11|\n",
      "|         1187|2015-12-12|other vegetables|       1|\n",
      "|         3037|2015-02-01|      whole milk|       0|\n",
      "|         4941|2015-02-14|      rolls/buns|       2|\n",
      "|         4501|2015-05-08|other vegetables|       1|\n",
      "|         3803|2015-12-23|      pot plants|      72|\n",
      "|         2762|2015-03-20|      whole milk|       0|\n",
      "|         4119|2015-02-12|  tropical fruit|       6|\n",
      "+-------------+----------+----------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 获取 Spark DataFrame 中的 item Description 字段\n",
    "items = [row.itemDescription for row in spark_df.collect()]\n",
    "cv = utils.Convert(items)\n",
    "\n",
    "# 将函数注册为 Spark UDF，返回值类型设为 Integer\n",
    "encoder_udf = F.udf(cv.encoder, IntegerType())\n",
    "\n",
    "# 在 DataFrame 上应用此函数\n",
    "spark_df = spark_df.withColumn(\"itemCode\", encoder_udf(F.col(\"itemDescription\")))\n",
    "\n",
    "# 将 Date 转换为 yyyy-MM-dd 格式\n",
    "spark_df = spark_df \\\n",
    "    .withColumn(\"Date\", F.date_format(F.to_date(F.col(\"Date\"), \"dd-MM-yyyy\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8e258-38e9-4c26-9624-f6dd1dbe136b",
   "metadata": {},
   "source": [
    "### 2.2 生成商品序列\n",
    "\n",
    "用 `Member_number` 分组，按 `Data` 从小到大的顺序，对 `itemCode` 排序，做成列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac24fcb-35bc-4fd9-a92a-da4afe6194f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:44.605050Z",
     "iopub.status.busy": "2024-09-05T08:48:44.604572Z",
     "iopub.status.idle": "2024-09-05T08:48:48.502579Z",
     "shell.execute_reply": "2024-09-05T08:48:48.499948Z",
     "shell.execute_reply.started": "2024-09-05T08:48:44.605015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Member_number|item_list                                                                                                                                                                      |\n",
      "+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1000         |2015-03-15,0;2015-05-27,3;2015-03-15,8;2015-07-24,48;2015-11-25,56;2015-07-24,13;2014-06-24,10;2014-06-24,0;2015-11-25,8;2015-05-27,67;2015-03-15,4;2015-03-15,65;2014-06-24,38|\n",
      "|1001         |2015-01-20,15;2015-04-14,29;2014-12-12,0;2015-05-02,17;2014-02-07,0;2015-01-20,3;2014-02-07,8;2015-01-20,17;2014-02-07,2;2015-05-02,24;2014-12-12,3;2015-04-14,23              |\n",
      "|1002         |2015-04-26,6;2014-04-26,0;2014-02-09,1;2015-04-26,41;2015-08-30,47;2014-02-09,27;2014-04-26,21;2015-08-30,42                                                                   |\n",
      "+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 将 Date 和 itemCode 用 `,` 拼接为 item_rec\n",
    "# 按 Member_number 分组，对 item_rec 去重，并用 `;` 拼接\n",
    "items_df = spark_df.filter((F.length(F.col(\"Date\")) > 1) & (F.col(\"itemCode\").isNotNull())) \\\n",
    "    .withColumn(\"item_rec\", F.concat_ws(\",\", F.col(\"Date\"), F.col(\"itemCode\"))) \\\n",
    "    .groupBy(\"Member_number\").agg(F.concat_ws(\";\", F.collect_set(\"item_rec\")) \\\n",
    "    .alias(\"item_list\"))\n",
    "\n",
    "items_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ea4cfc-306a-481d-a754-3ce9b504ab16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:48.506488Z",
     "iopub.status.busy": "2024-09-05T08:48:48.505940Z",
     "iopub.status.idle": "2024-09-05T08:48:48.529241Z",
     "shell.execute_reply": "2024-09-05T08:48:48.528081Z",
     "shell.execute_reply.started": "2024-09-05T08:48:48.506440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sorted_items(items: str,\n",
    "                 main_delimiter: str = ';',\n",
    "                 minor_delimiter: str = ','):\n",
    "    if len(items) == 0:\n",
    "        return []\n",
    "\n",
    "    item_list = []\n",
    "    for kv in items.split(main_delimiter):\n",
    "        kv_list = kv.split(minor_delimiter)\n",
    "        if len(kv_list) == 2:\n",
    "            item_list.append(kv_list)\n",
    "\n",
    "    sorted_list = sorted(item_list, key=lambda e: e[0])\n",
    "    sorted_items = [int(e[1]) for e in sorted_list]\n",
    "    return [[key for key, _ in groupby(sorted_items)]]\n",
    "\n",
    "sorted_items('2015-03-15,0;2015-05-27,3;2015-03-16,0;2014-03-16,1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62b8fa3-9e80-4e8d-9c62-546abc82ae3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:48.531314Z",
     "iopub.status.busy": "2024-09-05T08:48:48.530584Z",
     "iopub.status.idle": "2024-09-05T08:48:50.430008Z",
     "shell.execute_reply": "2024-09-05T08:48:50.424408Z",
     "shell.execute_reply.started": "2024-09-05T08:48:48.531283Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------------------------------------------------+\n",
      "|Member_number|sequence                                                                     |\n",
      "+-------------+-----------------------------------------------------------------------------+\n",
      "|1000         |[[10, 0, 38, 0, 8, 4, 65, 3, 67, 48, 13, 56, 8]]                             |\n",
      "|1001         |[[0, 8, 2, 0, 3, 15, 3, 17, 29, 23, 17, 24]]                                 |\n",
      "|1002         |[[1, 27, 0, 21, 6, 41, 47, 42]]                                              |\n",
      "|1003         |[[121, 45, 2, 5, 68, 2, 8]]                                                  |\n",
      "|1004         |[[11, 0, 6, 89, 90, 61, 31, 2, 76, 13, 1, 12, 5, 0, 10, 0, 69, 2, 31, 1, 56]]|\n",
      "|1005         |[[2, 15, 25]]                                                                |\n",
      "|1006         |[[0, 105, 7, 2, 113, 12, 140, 17, 31, 0, 14, 28, 64, 2]]                     |\n",
      "|1008         |[[5, 59, 4, 87, 20, 6, 3, 96, 32, 34, 104]]                                  |\n",
      "|1009         |[[73, 146, 4, 6, 10, 123, 16, 60, 4]]                                        |\n",
      "|1010         |[[36, 26, 11, 125, 54, 7, 55, 7, 17, 97, 11, 2]]                             |\n",
      "+-------------+-----------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 将函数注册为 Spark UDF，返回值类型设为 Integer\n",
    "sort_udf = F.udf(sorted_items, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "# 在 DataFrame 上应用此函数\n",
    "sorted_items_df = items_df.withColumn(\"item_list\", sort_udf(F.col(\"item_list\"))) \\\n",
    "    .withColumnRenamed(\"item_list\", \"sequence\")\n",
    "\n",
    "sorted_items_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c727d6-d51c-4334-ab76-eeed26aa6206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:45:31.650612Z",
     "iopub.status.busy": "2024-09-05T08:45:31.650049Z",
     "iopub.status.idle": "2024-09-05T08:45:31.655153Z",
     "shell.execute_reply": "2024-09-05T08:45:31.653970Z",
     "shell.execute_reply.started": "2024-09-05T08:45:31.650580Z"
    }
   },
   "source": [
    "### 2.3 计算频繁子序列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad122d8-881b-4b48-816f-66aa8215bfe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:50.433334Z",
     "iopub.status.busy": "2024-09-05T08:48:50.432153Z",
     "iopub.status.idle": "2024-09-05T08:48:50.446123Z",
     "shell.execute_reply": "2024-09-05T08:48:50.444844Z",
     "shell.execute_reply.started": "2024-09-05T08:48:50.433281Z"
    }
   },
   "outputs": [],
   "source": [
    "def freq2support(min_freq, spark_df):\n",
    "    support = min_freq / spark_df.count()\n",
    "    return support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91725316-e710-451b-aa71-4040bfac7ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:50.448399Z",
     "iopub.status.busy": "2024-09-05T08:48:50.447978Z",
     "iopub.status.idle": "2024-09-05T08:48:55.954279Z",
     "shell.execute_reply": "2024-09-05T08:48:55.953613Z",
     "shell.execute_reply.started": "2024-09-05T08:48:50.448358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minSupport: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 16:48:54 WARN PrefixSpan: Input data is not cached.        (2 + 6) / 8]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|sequence|freq|\n",
      "+--------+----+\n",
      "|  [[59]]| 172|\n",
      "|  [[42]]| 253|\n",
      "| [[102]]|  60|\n",
      "|  [[79]]|  89|\n",
      "|  [[82]]|  89|\n",
      "|  [[55]]| 205|\n",
      "|  [[85]]|  80|\n",
      "|  [[47]]| 228|\n",
      "|  [[15]]| 603|\n",
      "|  [[74]]| 101|\n",
      "+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_freq = 50\n",
    "minSupport = freq2support(min_freq=min_freq, spark_df=sorted_items_df)\n",
    "print(f'minSupport: {minSupport:.4f}')\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=minSupport,\n",
    "                        maxPatternLength=5,\n",
    "                        maxLocalProjDBSize=32000000)\n",
    "\n",
    "# Find frequent sequential patterns.\n",
    "pattern_df = prefixSpan.findFrequentSequentialPatterns(sorted_items_df)\n",
    "\n",
    "pattern_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755ea2c-e4ab-4f47-a2e7-d423595818f1",
   "metadata": {},
   "source": [
    "筛选大于特定长度的子序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5d0e72-7eac-40dc-8812-c8a7b648cf03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:55.955961Z",
     "iopub.status.busy": "2024-09-05T08:48:55.955623Z",
     "iopub.status.idle": "2024-09-05T08:48:56.406290Z",
     "shell.execute_reply": "2024-09-05T08:48:56.405453Z",
     "shell.execute_reply.started": "2024-09-05T08:48:55.955939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|     sequence|freq|\n",
      "+-------------+----+\n",
      "|[[0, 20, 22]]|  50|\n",
      "|[[0, 18, 24]]|  53|\n",
      "|[[0, 17, 18]]|  54|\n",
      "|[[0, 16, 20]]|  52|\n",
      "|[[0, 16, 18]]|  52|\n",
      "|[[0, 15, 23]]|  57|\n",
      "|[[0, 15, 19]]|  56|\n",
      "|[[0, 15, 17]]|  57|\n",
      "|[[0, 15, 16]]|  68|\n",
      "|[[0, 14, 25]]|  53|\n",
      "+-------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 长度至少为 3 的子序列\n",
    "filtered_patterns = pattern_df.filter(F.col(\"sequence\").getItem(0).isNotNull() \\\n",
    "                                      & (F.col(\"sequence\").getItem(0).getItem(2).isNotNull()) )\n",
    "filtered_patterns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcdfdf-d9f3-4c2f-bb71-5b057a11f563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:47:41.186952Z",
     "iopub.status.busy": "2024-09-05T08:47:41.186609Z",
     "iopub.status.idle": "2024-09-05T08:47:41.190207Z",
     "shell.execute_reply": "2024-09-05T08:47:41.189481Z",
     "shell.execute_reply.started": "2024-09-05T08:47:41.186926Z"
    }
   },
   "source": [
    "### 2.4 用子序列回溯原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7425d922-c09a-42ad-a263-6295d6375566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:56.407844Z",
     "iopub.status.busy": "2024-09-05T08:48:56.407545Z",
     "iopub.status.idle": "2024-09-05T08:48:56.669340Z",
     "shell.execute_reply": "2024-09-05T08:48:56.668549Z",
     "shell.execute_reply.started": "2024-09-05T08:48:56.407821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsequence: [0, 20, 22]\n"
     ]
    }
   ],
   "source": [
    "subsequence = filtered_patterns.limit(1) \\\n",
    "    .select(F.col(\"sequence\").getItem(0)) \\\n",
    "    .withColumnRenamed(\"sequence[0]\", \"seq\") \\\n",
    "    .collect()[0].seq\n",
    "\n",
    "print(f'subsequence: {subsequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b7bc47-28e8-43a6-9d1c-137376404659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:56.670928Z",
     "iopub.status.busy": "2024-09-05T08:48:56.670463Z",
     "iopub.status.idle": "2024-09-05T08:48:58.049210Z",
     "shell.execute_reply": "2024-09-05T08:48:58.047356Z",
     "shell.execute_reply.started": "2024-09-05T08:48:56.670889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------+\n",
      "|Member_number|sequence                                                  |\n",
      "+-------------+----------------------------------------------------------+\n",
      "|1825         |[[40, 36, 0, 38, 101, 3, 20, 14, 1, 28, 22, 50, 70]]      |\n",
      "|1916         |[[0, 9, 20, 4, 24, 11, 22]]                               |\n",
      "|2389         |[[34, 42, 27, 7, 36, 0, 86, 12, 50, 20, 22, 4, 62, 2, 69]]|\n",
      "+-------------+----------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 连续匹配\n",
    "def is_continuous_subsequence(sequence, subseq):\n",
    "    seq_len = len(sequence)\n",
    "    sub_len = len(subseq)\n",
    "    for i in range(seq_len - sub_len + 1):\n",
    "        if sequence[i:i+sub_len] == subseq:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 非连续匹配\n",
    "def is_subsequence(sequence, subseq):\n",
    "    it = iter(sequence)\n",
    "    return all(elem in it for elem in subseq)\n",
    "\n",
    "# 注册 UDF\n",
    "is_subsequence_udf = F.udf(lambda seq: is_subsequence(seq, subsequence), BooleanType())\n",
    "\n",
    "# 应用 UDF 来筛选 DataFrame\n",
    "sorted_items_df = sorted_items_df.withColumn(\"seq\", F.col(\"sequence\").getItem(0))\n",
    "result_df = sorted_items_df.filter(is_subsequence_udf(F.col(\"seq\")))\n",
    "result_df.select(F.col(\"Member_number\"), F.col(\"sequence\")).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9e18205-e739-488b-957c-011ca366600f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T08:48:58.051761Z",
     "iopub.status.busy": "2024-09-05T08:48:58.051312Z",
     "iopub.status.idle": "2024-09-05T08:48:58.065346Z",
     "shell.execute_reply": "2024-09-05T08:48:58.063926Z",
     "shell.execute_reply.started": "2024-09-05T08:48:58.051723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whole milk', 'domestic eggs', 'fruit/vegetable juice']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 子序列解码\n",
    "[cv[e] for e in subsequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc817a-4fd2-4060-b6ba-1f5a878ce5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
